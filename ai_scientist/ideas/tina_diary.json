[
    {
        "Name": "tina_lora_reasoning",
        "Title": "Tina: Tiny Reasoning Models via LoRA",
        "Short Hypothesis": "Applying parameter-efficient updates using Low-Rank Adaptation (LoRA) during reinforcement learning (RL) can efficiently instill strong reasoning abilities in tiny language models (1.5B parameters) with minimal computational cost, achieving performance competitive with fully-trained models.",
        "Related Work": "The paper discusses the landscape of open-source models aiming to replicate the reasoning capabilities of advanced models like o1. It reviews methods using reinforcement learning (RL) with verifiable rewards for reasoning tasks, highlighting techniques like reward models, rule-based verification, self-play, and algorithms like GRPO and Dr.GRPO. It also situates its approach within the context of parameter-efficient fine-tuning, specifically using Low-Rank Adaptation (LoRA) to modify model behavior by training only a small fraction of parameters, contrasting this with more expensive full-parameter training commonly used in other reasoning models.",
        "Abstract": "How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LORA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights & checkpoints.",
        "Experiments": [
            "Ideation Stage I: Initial exploration focused on Supervised Fine-Tuning (SFT) to instill reasoning, mimicking approaches like s1 via distillation.",
            "Ideation Stage II: Encountered high computational costs with SFT on large models and poor performance on smaller models, leading to a pivot towards focusing specifically on small (1.5B) models.",
            "Ideation Stage III: Considered Reinforcement Learning (RL) for its potential benefits over SFT but recognized the high cost and complexity of conventional full-parameter RL.",
            "Ideation Stage IV: Hypothesized that Parameter-Efficient Fine-Tuning (PEFT), specifically Low-Rank Adaptation (LoRA), could enable RL benefits cost-effectively on small models. Initial positive results with LoRA+RL confirmed its effectiveness for efficient reasoning enhancement, forming the core methodology.",
            "Execution Stage I: Re-evaluate baseline models (DeepSeek-R1-Distilled-Qwen-1.5B, STILL-3-1.5B-preview, DeepScaleR-1.5B-Preview, Open-RS1/2/3) using a consistent methodology (lighteval framework, vLLM engine, 2 L40S GPUs) on six reasoning benchmarks: AIME24/25, AMC23, MATH500, GPQA, and Minerva.",
            "Execution Stage II: Train Tina models by applying the final LoRA+RL methodology (GRPO-style algorithm) to the DeepSeek-R1-Distill-Qwen-1.5B base model, using datasets and setups from baseline models (STILL-3, DeepScaleR, Open-RS1/2/3).",
            "Execution Stage III: Evaluate Tina models on the same six reasoning benchmarks, comparing performance against baselines and demonstrating competitive or superior results achieved with significantly lower computational cost and training steps.",
            "Execution Stage IV: Conduct ablation studies varying: training dataset size and source (OpenR1, OpenThoughts, DeepScaleR, STILL-3, Open-S1, Open-RS, LIMR), LoRA learning rate (5e-7 to 5e-6), LoRA rank (4 to 64), and RL algorithm (GRPO vs. Dr.GRPO).",
            "Execution Stage V: Analyze the relationship between training FLOPs and reasoning performance, showing LoRA models achieve high performance with orders of magnitude fewer FLOPs than full-parameter models.",
            "Execution Stage VI: Investigate training dynamics (accuracy reward, format reward, completion length) to identify a phase transition related to format adaptation, noting that optimal performance often occurs around this transition."
        ],
        "Risk Factors and Limitations": [
            "The absolute reasoning performance ceiling might be limited by the small (1.5B) base model size compared to larger models.",
            "The evaluation focuses on mathematical and formal logic benchmarks; effectiveness on other reasoning domains like coding needs further investigation.",
            "Hyperparameter optimization was intentionally minimized for cost-effectiveness; further tuning specific to the LoRA-RL interplay might yield additional performance gains."
        ]
    }
]
